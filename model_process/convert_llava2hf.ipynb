{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Convert LLaVa-Onevision checkpoints from the original repository.\n",
    "\n",
    "URL: https://github.com/LLaVA-VL/LLaVA-NeXT/tree/main\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from accelerate import init_empty_weights\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from PIL import Image\n",
    "from safetensors import safe_open\n",
    "\n",
    "from transformers import (\n",
    "    AddedToken,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    LlavaOnevisionConfig,\n",
    "    LlavaOnevisionForConditionalGeneration,\n",
    "    LlavaOnevisionImageProcessor,\n",
    "    LlavaOnevisionProcessor,\n",
    "    LlavaOnevisionVideoProcessor,\n",
    "    SiglipVisionConfig,\n",
    ")\n",
    "\n",
    "\n",
    "KEYS_TO_MODIFY_MAPPING = {\n",
    "    \"model.vision_tower.\": \"\",\n",
    "    \"model.mm_projector\": \"multi_modal_projector\",\n",
    "    \"model\": \"model.model\",\n",
    "    \"vision_model.model\": \"vision_model\",\n",
    "    \"lm_head\": \"language_model.lm_head\",\n",
    "    \"model.model\": \"language_model.model\",\n",
    "    \"multi_modal_projector.0\": \"multi_modal_projector.linear_1\",\n",
    "    \"multi_modal_projector.2\": \"multi_modal_projector.linear_2\",\n",
    "    \"language_model.model.image_newline\": \"image_newline\",\n",
    "}\n",
    "\n",
    "chat_template = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n'}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>\\n' }}{% endfor %}{# Render all video then #}{% for content in message['content'] | selectattr('type', 'equalto', 'video') %}{{ '<video>\\n' }}{% endfor %}{# Render all text next #}{% if message['role'] != 'assistant' %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ content['text'] }}{% endfor %}{% else %}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{% generation %}{{ content['text'] }}{% endgeneration %}{% endfor %}{% endif %}{{'<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "\n",
    "\n",
    "def load_original_state_dict(model_id):\n",
    "    directory_path = snapshot_download(repo_id=model_id, allow_patterns=[\"*.safetensors\"])\n",
    "\n",
    "    original_state_dict = {}\n",
    "    for path in glob.glob(f\"{directory_path}/*\"):\n",
    "        if path.endswith(\".safetensors\"):\n",
    "            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n",
    "                for key in f.keys():\n",
    "                    original_state_dict[key] = f.get_tensor(key)\n",
    "\n",
    "    # tied wieghts so lm.head is not saved. Let's clone to load state dict\n",
    "    if \"lm_head.weight\" not in original_state_dict:\n",
    "        original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n",
    "\n",
    "    return original_state_dict\n",
    "\n",
    "\n",
    "def convert_state_dict_to_hf(state_dict):\n",
    "    new_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.endswith(\".inv_freq\"):\n",
    "            continue\n",
    "        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n",
    "            if key_to_modify in key:\n",
    "                key = key.replace(key_to_modify, new_key)\n",
    "\n",
    "        new_state_dict[key] = value.to(torch.float16)\n",
    "    return new_state_dict\n",
    "\n",
    "\n",
    "def load_image():\n",
    "    url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    return image\n",
    "\n",
    "\n",
    "def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n",
    "    # load original config\n",
    "    filepath = hf_hub_download(repo_id=model_id, filename=\"config.json\", repo_type=\"model\")\n",
    "    # read json\n",
    "    with open(filepath) as f:\n",
    "        data = json.load(f)\n",
    "        print(data)\n",
    "\n",
    "    if model_id in [\"lmms-lab/llava-onevision-qwen2-0.5b-ov\", \"lmms-lab/llava-onevision-qwen2-0.5b-si\"]:\n",
    "        text_model_id = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "    elif model_id in [\n",
    "        \"lmms-lab/llava-onevision-qwen2-7b-ov\",\n",
    "        \"lmms-lab/llava-onevision-qwen2-7b-si\",\n",
    "        \"lmms-lab/llava-onevision-qwen2-7b-ov-chat\",\n",
    "    ]:\n",
    "        text_model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "    elif model_id in [\n",
    "        \"lmms-lab/llava-onevision-qwen2-72b-ov\",\n",
    "        \"lmms-lab/llava-onevision-qwen2-72b-si\",\n",
    "        \"lmms-lab/llava-onevision-qwen2-72b-ov-chat\",\n",
    "    ]:\n",
    "        text_model_id = \"Qwen/Qwen2-72B-Instruct\"\n",
    "\n",
    "    vision_model_id = data[\"mm_vision_tower\"]\n",
    "    torch.set_default_dtype(torch.float16)\n",
    "    text_config = AutoConfig.from_pretrained(text_model_id)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_model_id, use_fast=True)\n",
    "    tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n",
    "    tokenizer.add_tokens(AddedToken(\"<video>\", special=True, normalized=False), special_tokens=True)\n",
    "\n",
    "    image_processor = LlavaOnevisionImageProcessor.from_pretrained(vision_model_id)\n",
    "    video_processor = LlavaOnevisionVideoProcessor.from_pretrained(vision_model_id)\n",
    "    processor = LlavaOnevisionProcessor(\n",
    "        tokenizer=tokenizer,\n",
    "        video_processor=video_processor,\n",
    "        image_processor=image_processor,\n",
    "        num_image_tokens=729,\n",
    "        vision_feature_select_strategy=\"full\",\n",
    "        chat_template=chat_template,\n",
    "    )\n",
    "\n",
    "    vision_config = SiglipVisionConfig(\n",
    "        hidden_size=1152,\n",
    "        image_size=384,\n",
    "        intermediate_size=4304,\n",
    "        num_attention_heads=16,\n",
    "        num_hidden_layers=26,  # drop the last layer\n",
    "        patch_size=14,\n",
    "        vision_use_head=False,  # no head\n",
    "    ).to_dict()\n",
    "\n",
    "    config = LlavaOnevisionConfig(\n",
    "        text_config=text_config.to_dict(),\n",
    "        vision_config=vision_config,\n",
    "        use_image_newline_parameter=True,\n",
    "    )\n",
    "\n",
    "    with init_empty_weights():\n",
    "        model = LlavaOnevisionForConditionalGeneration(config)\n",
    "\n",
    "    # load original state dict\n",
    "    state_dict = load_original_state_dict(model_id)\n",
    "    state_dict = convert_state_dict_to_hf(state_dict)\n",
    "    model.load_state_dict(state_dict, assign=True)\n",
    "    model.eval()\n",
    "\n",
    "    pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data\n",
    "    mu = torch.mean(pre_expansion_embeddings, dim=0).float()\n",
    "    n = pre_expansion_embeddings.size()[0]\n",
    "    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5 * sigma)\n",
    "\n",
    "    # We add an image token so we resize the model\n",
    "    # Pad to 64 for performance reasons\n",
    "    # Qwen-based models have extra unused space in the vocab size already, so no need to resize\n",
    "    pad_shape = 64\n",
    "    vocab_size = config.text_config.vocab_size\n",
    "    num_tokens = vocab_size + 2\n",
    "    model.resize_token_embeddings(num_tokens, pad_to_multiple_of=pad_shape)\n",
    "    model.language_model.model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n",
    "        tuple(\n",
    "            (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[vocab_size:].shape[0]))\n",
    "        ),\n",
    "        dim=0,\n",
    "    )\n",
    "    model.language_model.lm_head.weight.data[vocab_size:] = torch.stack(\n",
    "        tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[vocab_size:].shape[0]))),\n",
    "        dim=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Saving model and processor for {model_id} to {pytorch_dump_folder_path}\")\n",
    "    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n",
    "    model.save_pretrained(pytorch_dump_folder_path)\n",
    "    processor.save_pretrained(pytorch_dump_folder_path)\n",
    "\n",
    "    # Make space so we can load the model properly now.\n",
    "    del state_dict\n",
    "    gc.collect()\n",
    "\n",
    "    # Load everything back for inference tests in float32 because prev script was written as that\n",
    "    # Though it's mostly loaded in fp16 as original weights are in fp16\n",
    "    model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n",
    "        pytorch_dump_folder_path, torch_dtype=\"float16\", device_map=\"auto\"\n",
    "    )\n",
    "    processor = LlavaOnevisionProcessor.from_pretrained(pytorch_dump_folder_path)\n",
    "    device = model.device\n",
    "\n",
    "    # prepare inputs\n",
    "    image = load_image()\n",
    "    prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<image>\\nWhat is shown in this image?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch.float16)\n",
    "\n",
    "    # verify inputs\n",
    "    filepath = hf_hub_download(\n",
    "        repo_id=\"RaushanTurganbay/test-image\", filename=\"llava_onevision_pixel_values.pt\", repo_type=\"dataset\"\n",
    "    )\n",
    "    original_pixel_values = torch.load(filepath, map_location=\"cpu\")\n",
    "    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())\n",
    "\n",
    "    image_sizes = torch.tensor([[899, 1024]])\n",
    "    assert image_sizes[0].tolist() == inputs.image_sizes[0].tolist()\n",
    "\n",
    "    # verify single forward pass\n",
    "    print(\"Single forward pass\")\n",
    "    with torch.inference_mode():\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        print(\"Shape of logits:\", outputs.logits.shape)\n",
    "        print(\"First values of logits:\", outputs.logits[0, :3, :3])\n",
    "\n",
    "        if model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-si\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[-12.1953, -14.6797, -12.7891], [0.5840, -0.8467, 1.3799], [3.6055, 4.5430, 9.9062]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-ov\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[-12.0234, -14.3828, -12.7500], [2.3594, 1.0000, 3.9336], [3.6582, 4.7148, 9.1172]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-si\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[1.7656, 3.3418, 1.4033], [0.0757, 0.7427, 3.5098], [6.7109, 5.6797, 9.3828]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[1.8496, 3.4219, 1.3135], [3.0996, 3.0117, 3.1484], [4.2422, 4.7109, 9.9688]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-si\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[4.1875, 4.4883, 2.7910], [1.2949, 5.1328, 3.1582], [0.9390, 6.4531, 8.4375]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[4.2930, 4.7305, 2.7363], [1.7529, 5.0742, 3.9590], [1.3936, 6.3438, 9.3984]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov-chat\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[1.8662, 3.4316, 1.3174], [2.7109, 2.5488, 3.0117], [4.4648, 4.9648, 10.3359]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov-chat\":\n",
    "            # Not yet checked against reference\n",
    "            expected_slice = torch.tensor(\n",
    "                [[4.3086, 4.7344, 2.6953], [1.7090, 5.1719, 4.0234], [1.3057, 6.3438, 9.5469]],\n",
    "                dtype=torch.float32,\n",
    "                device=device,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_id} not supported\")\n",
    "\n",
    "        assert torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4)\n",
    "        print(\"Logits are ok!\")\n",
    "\n",
    "    # verify generation\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    generated_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    print(\"Generated text:\", repr(generated_text))\n",
    "\n",
    "    if model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-si\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart that shows the performance of different algorithms or models in a specific domain, such as image classification or natural language processing. The chart is color-coded to represent different algorithms, with each color corresponding to a specific algorithm. The algorithms are labeled as BLIP-2, InstructBLIP, Owen-VL-Chat, and LLaVA-1.5. The chart also includes a legend at the bottom that explains the color coding and the algorithms represented.\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-0.5b-ov\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into different categories, each represented by a different color and labeled with the name of the model or technique used. The models are evaluated based on their performance metrics, such as BLEU-2, InstructBLIP, Qwen-VL-Chat, and LLaVA-1.5. The radar chart helps to visualize the relative\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-si\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThis image is a radar chart that compares the performance of different models on various metrics. The models being compared are BLIP-2, InstructBLIP, and Qwen-VL-Chat. The metrics being compared are VQA, QA, GQA, VQA-av2, and VQA-av2. The chart shows that BLIP-2 performs the best on all metrics, followed by InstructBLIP and Qwen-VL-Chat.\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to compare multiple quantitative variables. Each axis represents a different variable, and the chart is filled with data points that represent the performance or values of different entities across these variables.\\n\\nIn this particular radar chart, the variables are represented on the axes, and the performance of different models or systems is shown by the lines connecting the data points. The models or systems are labeled along the bottom of the chart,\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-si\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, which is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The chart is used to compare the performance of different models or systems across various benchmarks or metrics.\\n\\nIn this specific radar chart, there are multiple axes, each representing a different benchmark or metric, such as VQA2, GQA, TextVQA, and others. The chart includes several colored lines\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart comparing the performance of different models on various multimodal benchmarks. The models compared are BLIP-2, InstructBLIP, POPE, QWen-VL-Chat, and LLava-1.5. The benchmarks include VQAv2, GQA, TextVQA, SQA-IMG, VizWiz, MM-IMDb, MM-VQA, MM-IMDb-CN, MM-IMDb-EN, MM-\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-7b-ov-chat\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image shows a radar chart, also known as a spider chart or a star chart, which is used to display multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. Each axis represents a different variable, and the values are plotted along these axes.\\n\\nIn this particular radar chart, there are multiple lines representing different models or systems, each distinguished by a different color and labeled with a name such as BLIP-2, In\"\n",
    "    elif model_id == \"lmms-lab/llava-onevision-qwen2-72b-ov-chat\":\n",
    "        expected_text = \"system\\nYou are a helpful assistant.\\nuser\\n\\nWhat is shown in this image?\\nassistant\\nThe image is a radar chart comparing the performance of different models on various multimodal benchmarks. The models compared are BLIP-2, InstructBLIP, POPE, QWen-VL-Chat, and LLava-1.5. The benchmarks include VQAv2, GQA, TextVQA, SQA-IMG, VizWiz, MM-IMDb, MM-VQA, MM-IMDb-CN, MM-IMDb-EN, MM-\"\n",
    "    else:\n",
    "        raise ValueError(f\"Model {model_id} not supported\")\n",
    "\n",
    "    assert generated_text == expected_text\n",
    "    print(\"Generated text is ok!\")\n",
    "\n",
    "    # verify batched generation\n",
    "    print(\"Batched generation...\")\n",
    "    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "    cats_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "    inputs = processor(\n",
    "        images=[image, cats_image],\n",
    "        text=[prompt, prompt],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(device, torch.float16)\n",
    "\n",
    "    for k, v in inputs.items():\n",
    "        print(k, v.shape)\n",
    "\n",
    "    print(\"Image sizes:\", inputs.image_sizes)\n",
    "\n",
    "    # make sure image_sizes are the same\n",
    "    # as otherwise batched generation doesn't work\n",
    "    inputs.image_sizes[1] = inputs.image_sizes[0]\n",
    "\n",
    "    print(\"Batched generation...\")\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=20,\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "    print(outputs)\n",
    "\n",
    "    if push_to_hub:\n",
    "        checkpoint_name = model_id.split(\"/\")[-1]\n",
    "        print(f\"Pushing to repo llava-hf/{checkpoint_name}-hf\")\n",
    "        model.push_to_hub(f\"llava-hf/{checkpoint_name}-hf\")\n",
    "        processor.push_to_hub(f\"llava-hf/{checkpoint_name}-hf\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model_id\",\n",
    "        help=\"Hub location of the model to convert\",\n",
    "        default=\"lmms-lab/llava-onevision-qwen2-0.5b-ov\",\n",
    "        choices=[\n",
    "            \"lmms-lab/llava-onevision-qwen2-0.5b-ov\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-0.5b-si\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-7b-si\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-7b-ov\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-72b-si\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-72b-ov\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-7b-ov-chat\",\n",
    "            \"lmms-lab/llava-onevision-qwen2-72b-ov-chat\",\n",
    "        ],\n",
    "        required=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pytorch_dump_folder_path\", type=str, required=True, help=\"Path to the output PyTorch model directory.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the 🤗 hub.\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    convert_llava_to_hf(args.model_id, args.pytorch_dump_folder_path, args.push_to_hub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
